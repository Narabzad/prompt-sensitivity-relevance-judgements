# Prompt Sensitivity in LLM-Based Relevance Judgments
This repository contains data and scripts for analyzing prompt sensitivity in LLM-based relevance judgments.

## Repository Structure
```
prompt-sensitivity-relevance-judgements/
â”œâ”€â”€ prompts.json             # The prompts used for LLM-based relevance judgment
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ topics/
â”‚   â”‚   â”œâ”€â”€ topics.filtered.dl20.txt    # Queries for DL 2020
â”‚   â”‚   â””â”€â”€ topics.filtered.dl21.txt    # Queries for DL 2021
â”‚   â”œâ”€â”€ qrels/
â”‚   â”‚   â”œâ”€â”€ qrels.dl20-passage.txt       # Human labels for DL 2020
â”‚   â”‚   â””â”€â”€ qrels.dl21-passage.txt       # Human labels for DL 2021
â”‚   â”œâ”€â”€ pairwise/
â”‚   â”‚   â”œâ”€â”€ pairs_20.txt                 # Selected passage pairs for DL 2020
â”‚   â”‚   â””â”€â”€ pairs_21.txt                 # Selected passage pairs for DL 2021
â”‚â”€â”€ judgments/
    â”œâ”€â”€ gpt-4o/
    â”‚   â”œâ”€â”€ binary/                  # Binary relevance judgments by GPT-4o
    â”‚   â”œâ”€â”€ graded/                  # Graded relevance judgments by GPT-4o
    â”‚   â””â”€â”€ pairwise/                # Pairwise relevance judgments by GPT-4o 
    â”œâ”€â”€ mistral/
    |   â”‚   â”œâ”€â”€ binary/                  # Binary relevance judgments by Mistral
    |   â”‚   â”œâ”€â”€ graded/                  # Graded relevance judgments by Mistral
    |   â”‚   â””â”€â”€ pairwise/                # Pairwise relevance judgments by Mistral
    â””â”€â”€ llama/
           â”œâ”€â”€ binary/                  # Binary relevance judgments by LLaMA
           â”œâ”€â”€ graded/                  # Graded relevance judgments by LLaMA
           â””â”€â”€ pairwise/                # Pairwise relevance judgments by LLaMA```

## Judgment File Naming Convention

Each **relevance judgment file** is named following this pattern: ```[model_name]_[judgment_type]dl[year][participant_id].txt```


Where:
- **`[model_name]`** â†’ The LLM used for judgments (`gpt-4o`, `mistral`, `llama`).
- **`[judgment_type]`** â†’ The type of relevance judgment:
  - `binary` â†’ Binary (0/1) judgments.
  - `graded` â†’ Graded relevance (0-3 scale).
  - `pairwise` â†’ Pairwise comparison between two passages.
- **`dl[year]`** â†’ The dataset year (`dl20` for 2020, `dl21` for 2021).
- **`[participant_id]`** â†’ The ID of the prompt used for the judgment.

### Example Filenames:
- `mistral_graded_dl20_1.txt` â†’ Graded relevance judgments by **Mistral** on **DL 2020**, using prompt **#1**.
- `gpt-4o_binary_dl21_5.txt` â†’ Binary judgments by **GPT-4o** on **DL 2021**, using prompt **#5**.

## Prompt Processing Details

- **Total Prompts:** Initially, there were **32 prompts** in `prompts.json`. 
  - **2 were test prompts** and excluded.
  - **30 prompts** were used in the experiments.
- **Final Processed Prompts:** After filtering, only **12 prompts from humans** and **12 prompts from LLMs** were considered valid for generating judgments.

## Missing Judgments Explanation

- Some judgment files may contain **fewer lines** than others.
- This happens because **some LLMs failed to judge certain passages** or returned incomplete results.
- If a file has missing judgments, it means the **LLM was unable to make a decision** for those cases.

## How to Use This Repository

1. **Explore `data/`** â†’ Contains queries, human labels, and LLM judgments.
2. **Check `prompts.json`** â†’ Defines the prompts used for relevance assessment.
3. **Analyze judgments in `data/judgments/`** â†’ Compare different LLM outputs.

For any issues, feel free to submit a pull request or open an issue. ðŸš€
